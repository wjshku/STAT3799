---
title: "Stat3799 Simulation"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(Deriv)
library(ggplot2)
```

1. General Setup and Testing

```{r}
alpha <- function(n, h, x, X, Y, k){
  temp <- 0
  for(i in 1:length(X)){
    temp <- temp + Y[i] * k((x - X[i])/h)/(n*h)
  }
  return(temp)
}

valpha <- Vectorize(alpha,SIMPLIFY = FALSE)

phat<- function(n, h, x, X, k){
  temp <- sum(k((x - X)/h)/(n*h))
  return(max(temp,0.001)) #this is to prevent the estimation from resulting in NA
}

vphat<- Vectorize(phat)

mhat <- function(n, h, x, X, Y, k){
  return(alpha(n,h,x,X,Y,k)/phat(n,h,x,X,k))
}

vmhat <- function(n, h, x, X, Y, k){
  return(valpha(n,h,x,X,Y,k)/vphat(n,h,x,X,k))
}

beta <- function(n, h, x, X, Y, k, m, g, Xu){
  Yu <- 1:length(Xu)
  for(i in 1:length(Xu)){
    Yu[i] <- mhat(n, h, Xu[i], X, Y, k)
  }
  return(alpha(m,g,x,Xu,Yu,k))
}

rhat <- function(n, h, x, X, Y, k, m, g, Xu){
  return(beta(n, h, x, X, Y, k, m, g, Xu)/phat(m,g,x,Xu,k))
}
```

Simulation functions
```{r}
simu_unif <- function(multi, n, m, xmax, sigmaep = 1){

    X <- runif(n*multi,xmax)
    Y <- mx(X) + rnorm(n, sd = sigmaep)

  return(data.frame(X, Y))
}

simu_norm <- function(multi, n, m, meanx, sigmax, sigmaep = 1){

    X <- rnorm(n*multi,mean = meanx,sd = sigmax)
    Y <- mx(X) + rnorm(n, sd = sigmaep)

  return(data.frame(X, Y))
}

nw_simu <- function(data, testd, n, h, k){
  
    esti_nw <- testd$Y
    esti <- data.frame()
    X <- data$X
    Y <- data$Y
    real <- testd$Y
    test <- testd$X
    
    for(j in 1:length(test)){
    #set.seed(i) # random, may exist better solutions
        esti_nw[j] <- mhat(n,h,test[j],X,Y,k)
    }
    
    MSE_NW <- mean((real - esti_nw)^2)
    #browser()

    esti <- rbind(esti,data.frame(X = test,esti_nw))

  return(list(MSE = MSE_NW, esti = esti))
  
}

#Self-Supervised
ss_simu <- function(data, data_un, testd, n, h, m, g, k){
  esti <- data.frame()
    esti_ss <- testd$Y

    X <- data$X
    Y <- data$Y
    Xu <- data_un$X
    
    test <- testd$X
    real <- testd$Y
    
    for(j in 1:length(test)){
    #set.seed(i) # random, may exist better solutions
        esti_ss[j] <- rhat(n,h,test[j],X,Y,k,m,g,Xu)
    }
    
    MSE_SS <- mean((real - esti_ss)^2)
    #browser()

    esti <- rbind(esti,data.frame(X = test,esti_ss))
  
  
  return(list(MSE = MSE_SS, esti = esti))
  
}

hy_simu <- function(testd, lambda, esti_nw, esti_ss){
  
  esti <- data.frame()
  
  real <- testd$Y
  
  esti_hy <- lambda * esti_nw + (1-lambda) * esti_ss
  
  esti <- rbind(esti,data.frame(X = testd$X ,esti_hy))
  
  MSE_HY <- mean((real - esti_hy)^2)

  return(list(MSE = MSE_HY, esti = esti))
}
```

Resampling
```{r}
data <- simu(2, para$n*2, para$m, sigmax = sigmax, sigmaep = sigmaep)
set.seed(101) # Set Seed so that same sample can be reproduced in future also
# Now Selecting 75% of data as sample from total 'n' rows of the data  
sample <- sample.int(n = nrow(data), size = floor(.75*nrow(data)), replace = F)
train_label <- data[sample[1:para$n],]
train_un <- data[sample[(1+para$n):(para$m+para$n)],]
test  <- data[-sample, ]
```

Bandwidth Search for NW Estimator
```{r}
  para <- paraset3(64,10/19,sigmaep,rk$value, muk$value,theta22$value)
  
  test <- seq(0,4*pi,length.out = 32)
  rounds <- 30
  
  set.seed(10)
  
  hlist <- seq(0.2,2,0.2)
  result_nw <- data.frame()
  for(i in 1:length(hlist)){
    h <- hlist[i]*para$h
    for(k in 1:rounds){
      MSE_NW <- nw_simu(train_label,test, para$n,h, k)$MSE
      
    }
    result_nw <- rbind(result_nw, data.frame(MSE_NW, h))
  }
  
result_nw
plot(mMSE_NW~h,data = result_nw,main = "Average MSE_NW")
min(result_nw$MSE_NW)
  
  hlist <- seq(0.5,2.5,0.5)
  glist <- seq(0.5,1.5,0.2)
  result_ss <- data.frame()
  for(i in 1:length(hlist)){
    for(j in 1:length(glist)){
    h <- hlist[i]*para$h
    g <- glist[j]*para$g
    MSE_SS <- ss_simu(train_label, train_un, test, para$n,h, para$m,g, k)$MSE
    result_ss <- rbind(result_ss, data.frame(mMSE_SS, h, g))
  }
  }
  
result_ss
min(result_ss$mMSE_SS)
plot(mMSE_SS~h,data = filter(result_ss, g == result_ss$g[1]),main = "g = 1")
plot(mMSE_SS~g,data = filter(result_ss, h == result_ss$h[1]),main = "h = 1")
```

Bandwidth Search for Hybrid Estimator
```{r}
hlist <- seq(0.5,2.5,0.5)
glist <- seq(0.5,2.5,0.2)

result_hy <- data.frame()
  for(i in 1:length(hlist)){
  for(j in 1:length(glist)){
    h <- hlist[i]*para$h
    g <- glist[j]*para$g
    para$lambda <- 1 + h^2/g^2
    nw_res <- nw_simu(train_label, test, para$n,h, k)
    ss_res <- ss_simu(train_label, train_un, test, para$n,h, para$m,g, k)
    
    hy_res <- hy_simu(test, para$lambda, nw_res$esti$esti_nw, ss_res$esti$esti_ss)
    
    result_hy <- rbind(result_hy, data.frame(MSE_HY = hy_res$MSE, h, g))
  }
  }

result_hy
min(result_hy$MSE_HY)
match(min(result_hy$MSE_HY),result_hy$MSE_HY)
plot(mMSE_HY~h,data = filter(result_hy, g == result_hy$g[32]),main = "g = 0.87")
plot(mMSE_HY~g,data = filter(result_hy, h == result_hy$h[4]),main = "h = 0.42")


```

Repeat experiment for numerous times
```{r}
set.seed(100)
rounds <- 10

hlist <- seq(0.5,2.5,0.5)
glist <- seq(0.5,4,0.5)

mMSE_NW <- 0
mMSE_HY <- 0
for(exp in 1:rounds){
  sample <- sample.int(n = nrow(data), size = floor(.75*nrow(data)), replace = F)
  train_label <- data[sample[1:para$n],]
  train_un <- data[sample[(1+para$n):(para$m+para$n)],]
  test  <- data[-sample, ]
  result_nw <- data.frame()
  for(i in 1:length(hlist)){
    h <- hlist[i]*para$h
    MSE_NW <- nw_simu(train_label, test, para$n,h, k)$MSE

    result_nw <- rbind(result_nw, data.frame(mMSE_NW = MSE_NW, h))
  }
  mMSE_NW <- mMSE_NW + result_nw$mMSE_NW
  
  result_hy <- data.frame()
  for(i in 1:length(hlist)){
  for(j in 1:length(glist)){
    h <- hlist[i]*para$h
    g <- glist[j]*para$g
    para$lambda <- 1 + h^2/g^2
    nw_res <- nw_simu(train_label, test, para$n,h, k)
    ss_res <- ss_simu(train_label, train_un, test, para$n,h, para$m,g, k)
    
    hy_res <- hy_simu(test, para$lambda, nw_res$esti$esti_nw, ss_res$esti$esti_ss)
    
    result_hy <- rbind(result_hy, data.frame(mMSE_HY = hy_res$MSE, h, g))
  }
  }
  mMSE_HY <- mMSE_HY + result_hy$mMSE_HY
}
result_nw$mMSE_NW <- mMSE_NW/rounds
result_hy$mMSE_HY <- mMSE_HY/rounds
```

Find the best h and g
```{r}
min(result_nw$mMSE_NW)
min(result_hy$mMSE_HY)

match(min(result_nw$mMSE_NW),result_nw$mMSE_NW)
match(min(result_hy$mMSE_HY),result_hy$mMSE_HY)

```

Draw the plot to compare NW and HY
```{r}
plot(mMSE_HY~h,data = filter(result_hy, g == result_hy$g[32]),main = "g = 2.17")
lines(mMSE_HY~h,data = filter(result_hy, g == result_hy$g[32]),col = "red")
lines(mMSE_NW~h,data = result_nw,col = "blue")
legend("topleft",col=c("blue","red"),cex = 0.5,legend=c("MSE_NW","MSE_HY"))

plot(mMSE_HY~h,data = filter(result_hy, g == result_hy$g[32]), type="b", pch=19, col="red", xlab="h", ylab="MSE",main = "g = 2.17")
# Add a line
lines(mMSE_NW~h,data = result_nw, pch=18, col="blue", type="b", lty=2)
# Add a legend
legend("topright", legend=c("MSE_HY", "MSE_NW"),col=c("red", "blue"), lty=1:2, cex=0.8)
```

2. Experiment 1
Statistical setting: X ~ N(0, 5), epsilon ~ N(0, 1)
Model: Y = mx(x) + epsilon, mx(x) = x^2
Data: We have in total 128 * 2 data points of (X,Y). Then I choose n = 32, 64, 128, then split the data into training and testing data sets. Training data points are further split into labeled and unlabeled data. 

Set up hyperparameters, distribution functions and kernel
```{r}
sigmax <- 5
p <- function(x) 1/sqrt(2*pi*sigmax^2)*exp(-0.5*(x-0)^2/sigmax^2)
p_1 <- Deriv(p)
p_2 <- Deriv(p_1)
q <- function(x) 1/sqrt(2*pi*sigmax^2)*exp(-0.5*(x-0)^2/sigmax^2)
q_1 <- Deriv(q)
q_2 <- Deriv(q_1)
mx <- function(x) x^2
mx_1 <- Deriv(mx)
mx_2 <- Deriv(mx_1)

k <- function(x) 1/sqrt(2*pi)*exp(-0.5*(x-0)^2)
```

Suppose we know the true distribution of X 
```{r}
sigmak <- integrate(function(x) k(x)^2*x^2, -Inf, Inf)
rk <- integrate(function(x) k(x)^2, -Inf, Inf)
muk <- integrate(function(x) k(x)*x^2, -Inf, Inf)
#rp <- integrate(function(x) p_2(x)^2, -Inf, Inf)
#rq <- integrate(function(x) q_2(x)^2, -Inf, Inf)

theta22 <- integrate(function(x) mx_2(x)^2*p(x), -Inf, Inf)
sigmaep <- 1
```

There are some results helping us calculate the optimal h and g for NW estimator. But knowledge of true distribution and true mx() may be required. Since we are doing grid search, this won't make a difference but just for the sake of convenience. 
```{r}
paraset1 <- function(n, m_order, sigmaep, rk, muk, theta22){
  m <- round(n^m_order)
  h <- (rk/muk^2*sigmaep/(theta22*n))^(1/5)
  g <- (rk/muk^2*sigmaep/(theta22*m))^(1/5)
  
  lambda <- 1 + h^2/g^2
  return(data.frame(n,m,h,g,lambda))
}

paraset2 <- function(n, m_order, sigmaep, rk, muk, test){
  m <- round(n^m_order)
  h <- (rk/muk^2*sigmaep/(mx_2(2)^2*p(test)*n))^(1/5)
  g <- (rk/muk^2*sigmaep/(mx_2(2)^2*p(test)*m))^(1/5)
  
  lambda <- 1 + h^2/g^2
  return(data.frame(n,m,h,g,lambda))
}

paraset_g <- function(n, m_order,rk, muk){
  m <- round(n^m_order)
  h <- (rk/muk^2/(n))^(1/5)
  g <- (rk/muk^2/(m))^(1/5)
    
  lambda <- 1 + h^2/g^2
  return(data.frame(n,m,h,g,lambda))
}
```

Calculate parameter
```{r}
para <- paraset_g(128,10/19,rk$value, muk$value)
```

Do simulations to find the (h,g) with smallest MSE: Resampling
```{r}
set.seed(50)

data <- simu_norm(para$n + para$m, 1, para$m, meanx = 0, sigmax = sigmax, sigmaep = sigmaep)

rounds <- 100

hlist <- seq(0.05,0.5,0.05)
glist <- seq(0.1,1.5,0.2)

mMSE_NW <- 0
mMSE_HY <- 0

set.seed(43)
record_nw <- matrix(nrow = length(hlist), ncol = rounds)#
record_hy <- matrix(nrow = length(hlist)*length(glist), ncol = rounds)#

for(exp in 1:rounds){
  sample <- sample.int(n = nrow(data), size = para$n, replace = T)
  train_label <- data[sample,]
  train_un <- data[-sample,]
  #test <- data[-sample,]
  test <- data.frame(X = 2, Y = mx(2))
  result_nw <- data.frame()
  for(i in 1:length(hlist)){
    #h <- hlist[i]*para$h
    h <- hlist[i]
    MSE_NW <- nw_simu(train_label, test, para$n,h, k)$MSE
    
    result_nw <- rbind(result_nw, data.frame(mMSE_NW = MSE_NW, h))
  }
  record_nw[,exp] <- result_nw$mMSE_NW #
  mMSE_NW <- mMSE_NW + result_nw$mMSE_NW
  
  result_hy <- data.frame()
  for(i in 1:length(hlist)){
    for(j in 1:length(glist)){
      h <- hlist[i]
      g <- glist[j]
      para$lambda <- 1 + h^2/g^2
      nw_res <- nw_simu(train_label, test, para$n,h, k)
      ss_res <- ss_simu(train_label, train_un, test, para$n,h, para$m,g, k)
      
      hy_res <- hy_simu(test, para$lambda, nw_res$esti$esti_nw, ss_res$esti$esti_ss)
      
      result_hy <- rbind(result_hy, data.frame(mMSE_HY = hy_res$MSE, h, g))
    }
  }
  record_hy[,exp] <- result_hy$mMSE_HY#
  mMSE_HY <- mMSE_HY + result_hy$mMSE_HY
}

result_nw$mMSE_NW <- mMSE_NW/rounds
result_hy$mMSE_HY <- mMSE_HY/rounds
```

Do simulations: True repitition
```{r}
rounds <- 100

hlist <- seq(0.5,2.5,0.5)
glist <- seq(2,10,1)

mMSE_NW <- 0
mMSE_HY <- 0

set.seed(20)
record_nw <- matrix(nrow = length(hlist), ncol = rounds)#
record_hy <- matrix(nrow = length(hlist)*length(glist), ncol = rounds)#

for(exp in 1:rounds){
  data <- simu_norm(para$m + para$n, 1, para$m, meanx = 0, sigmax = sigmax, sigmaep = sigmaep)
    
  train_label <- data[1:para$n,]
  train_un <- data[(1+para$n):(para$m+para$n),]
  #test <- data[-sample,]
  test <- data.frame(X = 2, Y = mx(2))
  result_nw <- data.frame()
  for(i in 1:length(hlist)){
    h <- hlist[i]*para$h
    MSE_NW <- nw_simu(train_label, test, para$n,h, k)$MSE
    
    result_nw <- rbind(result_nw, data.frame(mMSE_NW = MSE_NW, h))
  }
  record_nw[,exp] <- result_nw$mMSE_NW #
  mMSE_NW <- mMSE_NW + result_nw$mMSE_NW
  
  result_hy <- data.frame()
  for(i in 1:length(hlist)){
    for(j in 1:length(glist)){
      h <- hlist[i]*para$h
      g <- glist[j]*para$g
      para$lambda <- 1 + h^2/g^2
      nw_res <- nw_simu(train_label, test, para$n,h, k)
      ss_res <- ss_simu(train_label, train_un, test, para$n,h, para$m,g, k)
      
      hy_res <- hy_simu(test, para$lambda, nw_res$esti$esti_nw, ss_res$esti$esti_ss)
      
      result_hy <- rbind(result_hy, data.frame(mMSE_HY = hy_res$MSE, h, g))
    }
  }
  record_hy[,exp] <- result_hy$mMSE_HY#
  mMSE_HY <- mMSE_HY + result_hy$mMSE_HY
}

result_nw$mMSE_NW <- mMSE_NW/rounds
result_hy$mMSE_HY <- mMSE_HY/rounds
```

Checking whether the mMSE have already converged
```{r}
hist(record_nw[1,])
mean(record_nw)

hist(record_hy[45,])
mean(record_hy,na.rm = T)

rolling_mean_nw <- record_nw
for (i in 2:ncol(record_nw)){
  rolling_mean_nw[,i] <- rowMeans(record_nw[,1:i])
}
plot(rolling_mean_nw[1,])
lines(rolling_mean_nw[2,])
lines(rolling_mean_nw[3,])
lines(rolling_mean_nw[4,])
lines(rolling_mean_nw[5,])

rolling_mean_hy <- record_hy
for (i in 2:ncol(record_hy)){
  rolling_mean_hy[,i] <- rowMeans(record_hy[,1:i])
}

plot(rolling_mean_hy[HY_opt,],pch = 3, cex = 0.5)
lines(rolling_mean_hy[26,])
lines(rolling_mean_hy[27,])
lines(rolling_mean_hy[28,])
lines(rolling_mean_hy[21,])
```

Find the optimal h and g, and tuning the range of them
```{r}
min(result_nw$mMSE_NW,na.rm = TRUE)
min(result_hy$mMSE_HY,na.rm = TRUE)

NW_opt <- match(min(result_nw$mMSE_NW,na.rm = TRUE),result_nw$mMSE_NW)
HY_opt <-match(min(result_hy$mMSE_HY,na.rm = TRUE),result_hy$mMSE_HY)

plot(mMSE_NW~h,data = result_nw,col = "blue")

plot(mMSE_HY~h,type = "b",data = filter(result_hy, g == result_hy$g[HY_opt]),main = "g = 0.3")
plot(mMSE_HY~g,type = "b",data = filter(result_hy, h == result_hy$h[HY_opt]),main = "h = 0.1")
```

Visualize the performance of estimators
```{r}
plot(mMSE_HY~h,data = filter(result_hy, g == result_hy$g[HY_opt]), type="b", pch=19, col="red", xlab="h", ylab="MSE",main = "g = 0.5")
# Add a line
lines(mMSE_NW~h,data = result_nw, pch=18, col="blue", type="b", lty=2)
# Add a legend
legend("topright", legend=c("MSE_HY", "MSE_NW"),col=c("red", "blue"), lty=1:2, cex=0.8)
```

3. Confidence Interval
We need to estimate several unknown values in order to calculate the estimated variance.
First, we estimate derivative of mx(x) at x using mhat'(x).
```{r}
k_1 <- Deriv(k)

alpha_1 <- function(n, h, x, X, Y, k_1){
  temp <- sum(Y*k_1((x - X)/h)/(n*h^2))
  return(temp)
}

phat_1<- function(n, h, x, X, k_1){
  temp <- sum(k_1((x - X)/h)/(n*h^2))
  return(temp)
}

mhat_1 <- function(n, h, x, X, Y, k, k_1){
  return(alpha_1(n,h,x,X,Y,k_1)/phat(n,h,x,X,k) - alpha(n,h,x,X,Y,k)*phat_1(n,h,x,X,k_1)/phat(n,h,x,X,k)^2)
}

```

For the time being, we assume the error variance is known to us and do some simulations.
```{r}
esti_var <- function(n, h, m, g, mx_deriv1, sigmaep, px, qx, rk, sigmak, E_H_ratio = 1){
  var <- 1/(n*h*px)*sigmaep^2*rk + (h^4)/(m*g^3*qx)*E_H_ratio*mx_deriv1^2*sigmak
  return(var)
}

CI_1 <- function(level,adjust,para,train_label,train_un,test,sigmaep,rk,sigmak,E_H_ratio = 1){
  n <- para$n
  h <- para$h
  m <- para$m
  g <- para$g
  X <- train_label$X
  Y <- train_label$Y
  mx_deriv1 <- mhat_1(n,h,test$X,X,Y,k,k_1)
  px <- phat(n,h,test$X,X,k)
  qx <- phat(m,g,test$X,train_un$X,k)
  var_esti <- esti_var(n,h,m,g,mx_deriv1,sigmaep,px,qx,rk,sigmak,E_H_ratio = E_H_ratio)
  para$lambda <- 1 + h^2/g^2
  nw_esti <- mhat(n, h, test$X, X, Y, k)
  ss_esti <- rhat(n, h, test$X, X, Y, k, m, g, train_un$X)
  hy_esti <- hy_simu(test, para$lambda, nw_esti, ss_esti)$esti$esti_hy
  #browser()
  upper <- hy_esti + sqrt(var_esti)*qnorm(1-(1 - level)/2)*adjust
  lower <- hy_esti - sqrt(var_esti)*qnorm(1-(1 - level)/2)*adjust
  
  return(data.frame(h = h, g = g, lower, upper, esti = hy_esti, sd = sqrt(var_esti)))
}
```

Experiment4: Then we can find the 95% level of h in 0.1 - 2 and g in 0.1 - 2, after setting the parameters. And subsequently find out their coverage probability.

Setup parameters and sample.
```{r}
para <- paraset_g(128,10/19,rk$value, muk$value)
```


```{r}
set.seed(20)
data <- simu_norm(para$m + para$n, 1, para$m, meanx = 0, sigmax = sigmax, sigmaep = sigmaep)
```

Estimate CI for all (h,g)
```{r}
train_label <- data[1:para$n,]
train_un <- data[(1+para$n):(para$m+para$n),]

hlist <- seq(0.1,1,0.1)
glist <- 1/(seq(1.1,0.2,-0.1))

CI <- data.frame()
for(i in hlist){
  para$h <- i
  for(j in glist){
    para$g <- j
    CI <- rbind(CI,CI_1(adjust = 1,0.95,para,train_label,train_un,test,sigmaep,rk$value,sigmak$value))
  }
}

plot(CI$esti, xlab = "Combinations", ylab = "Estimates", ylim = c(-10,10))
points(CI$lower, col = "red")
points(CI$upper, col = "blue")
abline(h = 4, col = "black")
legend("topright", legend=c("upper bound","lower bound"),col=c("blue","red"), pch = c(1,1),cex=0.8)
```

Test the true coverage probability
```{r}
set.seed(10)

rounds <- 100

para$h <- 0.25 # result_hy$h[HY_opt]
para$g <- 0.5 # result_hy$g[HY_opt]

coverage <- data.frame()

for(i in 1:rounds){
  data <- simu_norm(para$m + para$n, 1, para$m, meanx = 0, sigmax = sigmax, sigmaep = sigmaep)

  train_label <- data[1:para$n,]
  train_un <- data[(1+para$n):(para$m+para$n),]
  
  coverage <- rbind(coverage,CI_1(adjust = 1,0.95,para,train_label,train_un,test,sigmaep,rk$value,sigmak$value))
}

coverage$cover <- (coverage$lower <= test$Y)*(coverage$upper >= test$Y)
mean(coverage$cover,na.rm = T)

```

Draw the plot to show how CI performs
```{r}
plot(coverage$lower, xlab = "Combinations", ylab = "Estimates",col="red", ylim = c(0,10))
points(coverage$upper, col = "blue")
lines(coverage$esti, lty = 2)
abline(h = 4, col = "black")
legend("topright", legend=c("upper bound","lower bound"),col=c("blue","red"), pch = c(1,1),cex=0.8)
```

```{r}
ggplot(data = coverage,aes(x = simulation,y = esti)) + 
  geom_line()+
  geom_hline(yintercept = 4)+
    geom_ribbon(aes(ymin=lower,ymax=upper),alpha=0.3)

```

Diagnostics
```{r}
var(coverage$esti)
mean(coverage$sd)^2
```

Find the relationship between bandwidth and coverage probability
```{r}
set.seed(10)

rounds <- 100

hlist <- seq(0.1,1,0.1)
glist <- seq(0.1,2,0.1)

coverage_all <- data.frame()

cover <- 1:rounds

for(i in 1:length(hlist)){
  para$h <- hlist[i]
  for(j in 1:length(glist)){
    para$g <- glist[j]
    for(ep in 1:rounds){
      data <- simu_norm(para$m + para$n, 1, para$m, meanx = 0, sigmax = sigmax, sigmaep = sigmaep)
    
      train_label <- data[1:para$n,]
      train_un <- data[(1+para$n):(para$m+para$n),]
      
      temp <- CI_1(adjust = 1,0.95,para,train_label,train_un,test,sigmaep,rk$value,sigmak$value)
      
      cover[ep] <- (temp$lower <= test$Y)*(temp$upper >= test$Y)
    }
    coverage_all <- rbind(coverage_all,data.frame(h = para$h, g = para$g, cover = mean(cover, na.rm = T)))
  }
}
```

Visualize the relationship between (h, g) and coverage probability
```{r}
plot(hlist,coverage_all$cover[1:length(hlist)],type = "n",pch = 1, ylab = "Coverage Probability", main = "(h,g) and Coverage Probability", ylim = c(0.3,1))
for(i in 1:length(glist)){
  lines(hlist,coverage_all$cover[(1+(i-1)*length(hlist)):(i*length(hlist))], col = i, pch = i ,type = "b")
}
legend("bottomleft", legend = glist,col=1:length(hlist), pch = 1:length(hlist),cex=0.8)
abline(h = 0.95, col = "black")
```


Next, we estimate the error variance. 
```{r}

```

avoid setting h too small when the data is sparse. Redefine phat so not fall below the constant.
Common approach to avoid Computational issue

Possible Conclusion: the choice of h and g matters and our choice should depend on our goal, i.e., whether we want better confidence interval or smaller mse.